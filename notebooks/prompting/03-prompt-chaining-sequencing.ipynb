{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompt Chaining and Sequencing Tutorial\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial delves into the powerful techniques of prompt chaining and sequencing when working with large language models. We'll demonstrate these concepts using Google's Gemini via OpenRouter and LangChain, showing you how to create sophisticated, multi-step AI-driven processes.\n",
        "\n",
        "## Motivation\n",
        "\n",
        "As AI applications grow in complexity, it becomes crucial to break down intricate tasks into manageable steps. Prompt chaining and sequencing provide a framework for guiding language models through a series of interconnected prompts, resulting in more structured and controlled outputs. This approach is invaluable for tasks requiring multiple stages of processing or decision-making.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. **Basic Prompt Chaining**: Linking outputs from one prompt to inputs of another.\n",
        "2. **Sequential Prompting**: Designing a logical progression of prompts to guide the AI through multi-step processes.\n",
        "3. **Dynamic Prompt Generation**: Utilizing the output of one prompt to adaptively create the next prompt.\n",
        "4. **Error Handling and Validation**: Implementing safeguards and quality checks within the prompt chain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Let's start by importing the necessary libraries and setting up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from os import getenv\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize the language model\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=getenv(\"OPENROUTER_API_KEY\"),\n",
        "    openai_api_base=getenv(\"OPENROUTER_BASE_URL\"),\n",
        "    model_name=\"google/gemini-flash-1.5\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Prompt Chaining\n",
        "\n",
        "Let's start with a simple example of prompt chaining. We'll create two prompts: one to generate a short story, and another to summarize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Story: The crimson sun bled across Xylos, casting long shadows from the bioluminescent fungi forests.  Elara, her cybernetic arm humming faintly, crept through the undergrowth, her mission to retrieve the stolen stardust crystals before the Kryll Swarm reached them.  Failure meant not only the death of her people, but the extinction of all life on Xylos.  The crystals pulsed with a faint, ethereal light, a beacon and a deadly threat.\n",
            "\n",
            "\n",
            "Summary: On the dying planet Xylos, Elara raced against a deadly swarm to retrieve stolen stardust crystals, the fate of her people and all life hanging in the balance.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define prompt templates\n",
        "story_prompt = PromptTemplate(\n",
        "    input_variables=[\"genre\"], template=\"Write a short {genre} story in 3-4 sentences.\"\n",
        ")\n",
        "\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"story\"],\n",
        "    template=\"Summarize the following story in one sentence:\\n{story}\",\n",
        ")\n",
        "\n",
        "# Chain the prompts\n",
        "\n",
        "\n",
        "def story_chain(genre):\n",
        "    \"\"\"Generate a story and its summary based on a given genre.\n",
        "\n",
        "    Args:\n",
        "        genre (str): The genre of the story to generate.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the generated story and its summary.\n",
        "    \"\"\"\n",
        "    story = (story_prompt | llm).invoke({\"genre\": genre}).content\n",
        "    summary = (summary_prompt | llm).invoke({\"story\": story}).content\n",
        "    return story, summary\n",
        "\n",
        "\n",
        "# Test the chain\n",
        "genre = \"science fiction\"\n",
        "story, summary = story_chain(genre)\n",
        "print(f\"Story: {story}\\n\\nSummary: {summary}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sequential Prompting\n",
        "\n",
        "Now, let's create a more complex sequence of prompts for a multi-step analysis task. We'll analyze a given text for its main theme, tone, and key takeaways."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Theme: The main theme is the **dual nature of artificial intelligence and the need for responsible development**.  The text highlights both the potential benefits and the potential risks of AI, emphasizing the importance of careful consideration and proactive measures to mitigate its negative consequences.\n",
            "\n",
            "\n",
            "Tone: The overall tone of the text is **cautiously optimistic**.  While acknowledging the potential benefits and revolutionary aspects of AI, it emphasizes the need for careful consideration of the associated risks and ethical implications.  It's not purely positive nor purely negative, but rather a balanced perspective that leans towards a responsible and measured approach to AI's development.\n",
            "\n",
            "\n",
            "Takeaways: The key takeaways from the text are:\n",
            "\n",
            "* **AI's Dual Nature:** Artificial intelligence presents both immense potential benefits (revolutionizing industries, improving daily life) and significant risks (privacy concerns, job displacement, potential for misuse).  This duality is central to understanding AI's impact.\n",
            "\n",
            "* **Need for Responsible Development:**  The text strongly emphasizes the critical need for a cautious and proactive approach to AI development.  This includes careful consideration of ethical implications and proactive measures to mitigate potential negative consequences.\n",
            "\n",
            "* **Balanced Perspective:** The overall message isn't alarmist or overly enthusiastic. It advocates for a measured and responsible approach, acknowledging both the exciting possibilities and the serious challenges posed by AI.\n",
            "\n",
            "* **Urgency of Action:** The phrase \"As we stand on the brink of this technological revolution\" highlights the time-sensitive nature of addressing the ethical and societal implications of AI.  We must act now to ensure its responsible development.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define prompt templates for each step\n",
        "theme_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Identify the main theme of the following text:\\n{text}\",\n",
        ")\n",
        "\n",
        "tone_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Describe the overall tone of the following text:\\n{text}\",\n",
        ")\n",
        "\n",
        "takeaway_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\", \"theme\", \"tone\"],\n",
        "    template=\"Given the following text with the main theme '{theme}' and tone '{tone}', what are the key takeaways?\\n{text}\",\n",
        ")\n",
        "\n",
        "\n",
        "def analyze_text(text):\n",
        "    \"\"\"Perform a multi-step analysis of a given text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to analyze.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the theme, tone, and key takeaways of the text.\n",
        "    \"\"\"\n",
        "    theme = (theme_prompt | llm).invoke({\"text\": text}).content\n",
        "    tone = (tone_prompt | llm).invoke({\"text\": text}).content\n",
        "    takeaways = (\n",
        "        (takeaway_prompt | llm)\n",
        "        .invoke({\"text\": text, \"theme\": theme, \"tone\": tone})\n",
        "        .content\n",
        "    )\n",
        "    return {\"theme\": theme, \"tone\": tone, \"takeaways\": takeaways}\n",
        "\n",
        "\n",
        "# Test the sequential prompting\n",
        "sample_text = \"The rapid advancement of artificial intelligence has sparked both excitement and concern among experts. While AI promises to revolutionize industries and improve our daily lives, it also raises ethical questions about privacy, job displacement, and the potential for misuse. As we stand on the brink of this technological revolution, it's crucial that we approach AI development with caution and foresight, ensuring that its benefits are maximized while its risks are minimized.\"\n",
        "\n",
        "analysis = analyze_text(sample_text)\n",
        "for key, value in analysis.items():\n",
        "    print(f\"{key.capitalize()}: {value}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dynamic Prompt Generation\n",
        "\n",
        "In this section, we'll create a dynamic question-answering system that generates follow-up questions based on previous answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q1: What are the potential applications of quantum computing?\n",
            "A1: Drug discovery, materials science, financial modeling, cryptography, optimization problems, and artificial intelligence.\n",
            "\n",
            "\n",
            "Q2: What are the biggest current challenges hindering the wider application of quantum computing in these fields?\n",
            "\n",
            "A2: High cost, qubit fragility, limited scalability, and lack of error correction are the biggest current challenges hindering wider application of quantum computing.\n",
            "\n",
            "\n",
            "Q3: Given the limitations of high cost, qubit fragility, limited scalability, and lack of error correction, what are the most promising research avenues currently being pursued to overcome these challenges and accelerate the adoption of quantum computing?\n",
            "\n",
            "A3: Promising avenues include developing more stable qubits (e.g., topological qubits), improving fabrication techniques for scalability (e.g., silicon-based qubits), and advancing error correction codes and fault-tolerant architectures.\n",
            "\n",
            "\n",
            "Q4: Given the diverse approaches to qubit technology (e.g., superconducting, trapped ion, photonic), how do researchers determine which qubit modality is most likely to overcome the challenges of cost, fragility, scalability, and error correction in the near to mid-term, and what are the key criteria for this assessment?\n",
            "\n",
            "A4: Researchers assess qubit modalities based on their progress towards scalable fabrication, coherence times, gate fidelity, and error correction implementation.  No single modality is definitively \"best,\"  but advancements in these criteria, coupled with cost projections, inform near-to-mid-term viability assessments.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define prompt templates\n",
        "answer_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Answer the following question concisely:\\n{question}\",\n",
        ")\n",
        "\n",
        "follow_up_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"answer\"],\n",
        "    template=\"Based on the question '{question}' and the answer '{answer}', generate a relevant follow-up question.\",\n",
        ")\n",
        "\n",
        "\n",
        "def dynamic_qa(initial_question, num_follow_ups=3):\n",
        "    \"\"\"Conduct a dynamic Q&A session with follow-up questions.\n",
        "\n",
        "    Args:\n",
        "        initial_question (str): The initial question to start the Q&A session.\n",
        "        num_follow_ups (int): The number of follow-up questions to generate.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries containing questions and answers.\n",
        "    \"\"\"\n",
        "    qa_chain = []\n",
        "    current_question = initial_question\n",
        "\n",
        "    for _ in range(num_follow_ups + 1):  # +1 for the initial question\n",
        "        answer = (answer_prompt | llm).invoke({\"question\": current_question}).content\n",
        "        qa_chain.append({\"question\": current_question, \"answer\": answer})\n",
        "\n",
        "        if _ < num_follow_ups:  # Generate follow-up for all but the last iteration\n",
        "            current_question = (\n",
        "                (follow_up_prompt | llm)\n",
        "                .invoke({\"question\": current_question, \"answer\": answer})\n",
        "                .content\n",
        "            )\n",
        "\n",
        "    return qa_chain\n",
        "\n",
        "\n",
        "# Test the dynamic Q&A system\n",
        "initial_question = \"What are the potential applications of quantum computing?\"\n",
        "qa_session = dynamic_qa(initial_question)\n",
        "\n",
        "for i, qa in enumerate(qa_session):\n",
        "    print(f\"Q{i+1}: {qa['question']}\")\n",
        "    print(f\"A{i+1}: {qa['answer']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Handling and Validation\n",
        "\n",
        "In this final section, we'll implement error handling and validation in our prompt chains to make them more robust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final result for topic '\u0110i\u1ec7n Bi\u00ean Ph\u1ee7': 1954\n"
          ]
        }
      ],
      "source": [
        "# Define prompt templates\n",
        "generate_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Generate a 4-digit number related to the topic: {topic}. Respond with ONLY the number, no additional text.\",\n",
        ")\n",
        "\n",
        "validate_prompt = PromptTemplate(\n",
        "    input_variables=[\"number\", \"topic\"],\n",
        "    template=\"Is the number {number} truly related to the topic '{topic}'? Answer with 'Yes' or 'No' and explain why.\",\n",
        ")\n",
        "\n",
        "\n",
        "def extract_number(text):\n",
        "    \"\"\"Extract a 4-digit number from the given text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to extract the number from.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The extracted 4-digit number, or None if no valid number is found.\n",
        "    \"\"\"\n",
        "    match = re.search(r\"\\b\\d{4}\\b\", text)\n",
        "    return match.group() if match else None\n",
        "\n",
        "\n",
        "def robust_number_generation(topic, max_attempts=3):\n",
        "    \"\"\"Generate a topic-related number with validation and error handling.\n",
        "\n",
        "    Args:\n",
        "        topic (str): The topic to generate a number for.\n",
        "        max_attempts (int): Maximum number of generation attempts.\n",
        "\n",
        "    Returns:\n",
        "        str: A validated 4-digit number or an error message.\n",
        "    \"\"\"\n",
        "    for attempt in range(max_attempts):\n",
        "        try:\n",
        "            response = (generate_prompt | llm).invoke({\"topic\": topic}).content\n",
        "            number = extract_number(response)\n",
        "\n",
        "            if not number:\n",
        "                raise ValueError(\n",
        "                    f\"Failed to extract a 4-digit number from the response: {response}\"\n",
        "                )\n",
        "\n",
        "            # Validate the relevance\n",
        "            validation = (\n",
        "                (validate_prompt | llm)\n",
        "                .invoke({\"number\": number, \"topic\": topic})\n",
        "                .content\n",
        "            )\n",
        "            if validation.lower().startswith(\"yes\"):\n",
        "                return number\n",
        "            else:\n",
        "                print(\n",
        "                    f\"Attempt {attempt + 1}: Number {number} was not validated. Reason: {validation}\"\n",
        "                )\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
        "\n",
        "    return \"Failed to generate a valid number after multiple attempts.\"\n",
        "\n",
        "\n",
        "# Test the robust number generation\n",
        "topic = \"\u0110i\u1ec7n Bi\u00ean Ph\u1ee7\"\n",
        "result = robust_number_generation(topic)\n",
        "print(f\"Final result for topic '{topic}': {result}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
